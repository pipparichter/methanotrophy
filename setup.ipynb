{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('./src')\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import NoReturn\n",
    "import wonderwords\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/prichter/Documents/trophy/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "A couple of notes about the data. First, there are two samples (HDK22-KML-sand-dry and HDK22-KML-sand-wet) which are not from California soil. These can be discarded. Additionally, samples HDK-DNAexNegLot169030916-30cyc and HDK-MAR-PCR-BLANK can be discarded. They seem to be some kind of artifact of the sequencing.\n",
    "\n",
    "Another quirk to note is the fact that serial codes provided in the `metadata.tsv` file are not unique -- the serial codes for each of the surface and deep samples are the same. Hannah said that it is OK to merge the counts for these samples, and that it will likely produce better results. I should also note that the other metadata fields for samples with the same serial code (`flux_ch4`, `temp_air`, etc.) seem to be the same (or similar).\n",
    "\n",
    "*In the R code which Hannah used to generate the original NMDS plots, surface and deep samples were not combined. I doubt that this is the source of the differences in the NMDS plots, but it might be worth double-checking.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nonzero_row_sums(df:pd.DataFrame):\n",
    "    '''Check to make sure that all rows have at least a count of 1 across all columns.'''\n",
    "    counts = df.values\n",
    "    n_rows = len(counts)\n",
    "    n_zeros = n_rows - np.count_nonzero(np.sum(counts, axis=1))\n",
    "    assert n_zeros == 0, f'check_nonzero_row_sums: {n_zeros} zero rows were found in the input DataFrame.'\n",
    "\n",
    "def check_nonzero_col_sums(df:pd.DataFrame):\n",
    "    '''Check to make sure that all columns have at least a count of 1 across all rows.'''\n",
    "    counts = df.values\n",
    "    n_cols = counts.shape[1]\n",
    "    n_zeros = n_cols - np.count_nonzero(np.sum(counts, axis=0))\n",
    "    assert n_zeros == 0, f'check_nonzero_col_sums: {n_zeros} zero columns were found in the input DataFrame.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_from_metadata(path:str) -> pd.DataFrame:\n",
    "    '''Load the sample metadata. Keep the duplicate serial codes so that sample names can be mapped to serial codes\n",
    "    later on during preprocessing.'''\n",
    "    metadata_df = pd.read_csv(path)\n",
    "    metadata_df.columns = [col.lower() for col in metadata_df.columns]\n",
    "    metadata_df = metadata_df[~metadata_df.serial_code.isin(['PCR blank', 'Extr blank', 'KML'])] # Drop the outliers. \n",
    "    metadata_df.serial_code = metadata_df.serial_code.apply(int) # Convert serial code to integers.\n",
    "    metadata_df.soil_depth = metadata_df.soil_depth.str.lower()\n",
    "    return metadata_df\n",
    "\n",
    "\n",
    "def df_from_taxonomy(path:str) -> pd.DataFrame:\n",
    "    '''Load the ASV taxonomy information. The taxonomy file contains non-prokaryotes (e.g. nematodes and Eukaryotes), which \n",
    "    we keep in for the same of completeness.'''\n",
    "    taxonomy_df = pd.read_csv(path, delimiter='\\t', index_col=0)\n",
    "    # Handled the unclassified taxa (they are entered as {some category}_unclassified)\n",
    "    taxonomy_df = taxonomy_df.map(lambda s : s.replace('unclassified_', ''))\n",
    "\n",
    "    for col in taxonomy_df.columns:\n",
    "        # TODO: Might need a more general way to handle the Eukaryotes. \n",
    "\n",
    "        # Some of the nematode taxonomy labels are slightly irregular, as they contain clade information. The clades are discarded\n",
    "        # for the same of simplicity (probably don't need much taxonomical resolution for the nematodes.)\n",
    "        taxonomy_df[col] = taxonomy_df[col].apply(lambda s : 'Amorphea' if ('Amorphea' in s) else s)\n",
    "        # SAR or Harosa is a highly diverse clade of eukaryotes, often considered a supergroup, that includes stramenopiles, alveolates,\n",
    "        # and rhizarians (all Eukaryotes). Also make an exception for taxonomy labels of this kind. \n",
    "        taxonomy_df[col] = taxonomy_df[col].apply(lambda s : 'Harosa' if ('SAR' in s) else s)\n",
    "        taxonomy_df[col] = taxonomy_df[col].apply(lambda s : 'Archaeplastida' if ('Archaeplastida' in s) else s)\n",
    "\n",
    "        p = '([a-zA-Z0-9_]+)_([0-9]+)' # Pattern to match, all taxa ending in _{number}{number}\n",
    "        # Create a new \"sub\" column for more taxonomical resolution if a numerical sub-category is given. \n",
    "        taxonomy_df[col + '_sub'] = taxonomy_df[col].apply(lambda s : int(re.match(p, s).group(2)) if not (re.match(p, s) is None) else 0)\n",
    "        taxonomy_df[col] = taxonomy_df[col].apply(lambda s : re.match(p, s).group(1) if not (re.match(p, s) is None) else s)\n",
    "    # Set the index name for merging later on. \n",
    "    taxonomy_df.index.name = 'asv'\n",
    "    return taxonomy_df\n",
    "\n",
    "\n",
    "def df_from_counts(path:str) -> pd.DataFrame:\n",
    "    '''Load the ASV count information.'''\n",
    "    counts_df = pd.read_csv(path, delimiter='\\t', index_col=0)\n",
    "    # Confirm that none of the samples or columns are zero. \n",
    "    check_nonzero_col_sums(counts_df)\n",
    "    check_nonzero_row_sums(counts_df)\n",
    "    # Each column in the count_df corresponds to a sample label. \n",
    "    # Throw out the columns for the artifact samples, and those which are not from California soil. \n",
    "    counts_df = counts_df.drop(columns=['HDK-DNAexNegLot169030916-30cyc', 'HDK-MAR-PCR-BLANK'])\n",
    "    counts_df = counts_df.drop(columns=['HDK22-KML-sand-dry', 'HDK22-KML-sand-wet'])    \n",
    "    # Confirm that none of the samples or columns are zero after removing these samples.\n",
    "    # Turns out removing these samples results in some ASV counts becoming zero. Remove those ASVs. \n",
    "    counts_df = counts_df.iloc[counts_df.values.sum(axis=1) > 0]\n",
    "    check_nonzero_col_sums(counts_df)\n",
    "    check_nonzero_row_sums(counts_df)\n",
    "\n",
    "    counts_df['asv'] = counts_df.index # Create a new column from the ASV values in the index. \n",
    "    counts_df = counts_df.melt(value_name='count', id_vars='asv', var_name='sample') # Convert columns to a categorical 'sample' variable.\n",
    "\n",
    "    return counts_df.set_index('asv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = df_from_counts(os.path.join(DATA_DIR, 'ASVs_counts_Mar2023.tsv'))\n",
    "taxonomy_df = df_from_taxonomy(os.path.join(DATA_DIR, 'ASVs_taxonomy_Mar2023.tsv'))\n",
    "metadata_df = df_from_metadata(f'{DATA_DIR}/metadata.csv')\n",
    "\n",
    "sample_to_serial_code_map = {row.sample:row.serial_code for row in metadata_df.itertuples()}\n",
    "counts_df['serial_code'] = counts_df['sample'].apply(lambda sample : sample_to_serial_code_map[sample])\n",
    "counts_df = counts_df.drop(columns='sample')\n",
    "\n",
    "# Now that sample names have been mapped to serial codes, we can merge the rows with the same serial codes in the count DataFrame. \n",
    "counts_df = counts_df.groupby(by=['serial_code', 'asv']).sum() # Sum up over the rows with duplicate serial codes. \n",
    "\n",
    "# Combine the data across files into a single DataFrame. Drop the samples column, which is redundant with the serial code. \n",
    "df = counts_df.merge(taxonomy_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "df.to_csv(f'{DATA_DIR}/data.csv', index=True) # # Save the cleaned-up data to a CSV file. 'asv' is the index, so make sure to include when writing. \n",
    "# Keep duplicate rows in the metadata in case I need to re-generate a sample-to-serial code map again, for whatever reason.\n",
    "metadata_df.to_csv(f'{DATA_DIR}/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data processing\n",
    "\n",
    "It seems useful to add some additional checks to make sure no information was lost during the data preprocessing stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df(df:pd.DataFrame):\n",
    "    '''Check the cleaned-up data in the DataFrame (merged with taxonomy) to make sure it is in-line with the \n",
    "    original counts data.'''\n",
    "    # In the original DataFrame, ASVs are rows. \n",
    "    original_count_df = pd.read_csv(f'{DATA_DIR}/ASVs_counts_Mar2023.tsv', delimiter='\\t', index_col=0)\n",
    "    # Dropping these samples is the extent of the data preprocessing performed in the Rmd file Hannah sent. \n",
    "    original_count_df = original_count_df.drop(columns=['HDK-DNAexNegLot169030916-30cyc', 'HDK-MAR-PCR-BLANK', 'HDK22-KML-sand-dry', 'HDK22-KML-sand-wet'])\n",
    "\n",
    "    # Remove the taxonomy information while dealing with counts. \n",
    "    df = df['count']\n",
    "    \n",
    "    # Make sure the total count in the cleaned-up data is the same. \n",
    "    assert np.sum(df.values) == np.sum(original_count_df.values), 'Total count in cleaned-up data differs from the original dataset.'\n",
    "\n",
    "    # Make sure the per-ASV count is the same across datasets. There's not a good way to check the per-sample count,\n",
    "    # as samples were merged by serial code. \n",
    "    asvs = set(df.index.get_level_values('asv')) # Extract all ASVs from multi-index.\n",
    "    for asv in tqdm(asvs, desc='Checking ASV counts.'): # Do this by for loop, as some ASVs have been removed (became zero when samples were removed).\n",
    "        original_per_asv_counts = original_count_df[original_count_df.index == asv].values.sum()\n",
    "        per_asv_counts = df[df.index.get_level_values('asv') == asv].sum().item()\n",
    "        assert original_per_asv_counts == per_asv_counts, f'Count for ASV {asv} differs from the original dataset.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking ASV counts.:  10%|▉         | 9404/95207 [13:56:05<16:49:22,  1.42it/s]       "
     ]
    }
   ],
   "source": [
    "check_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial data\n",
    "\n",
    "For testing the functionality of functions included in this software package, it is also necessary to generate some psuedo-count data, as well as artificial metadata. The artificial taxa counts are sampled from a normal distribution. Random taxa names for each taxonomy level (domain, kingdom, phylum, class, order, family, genus, species) are also generated. Metadata for each sample are also randomly generated, containing both categorical and continuous fake environmental variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artificial_taxonomy(n_asvs:int, n:int=3):\n",
    "    '''Generate an artificial hierarchical taxonomy for n_asvs ASVs. The input n\n",
    "    specifies the number of branches from each node in the artificial hierarchy.'''\n",
    "    r = wonderwords.RandomWord() # Instantiate the random word generator. \n",
    "\n",
    "    levels = ['domain', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "    df = pd.DataFrame(columns=['asv'] + levels) # Instantiate a DataFrame with the necessary columns.     \n",
    "    df['asv'] = [f'ASV_{i}' for i in range(n_asvs)] # Add the ASVs to the DataFrame. \n",
    "\n",
    "    # Assign domains to each ASV to initialize the hierarchy. \n",
    "    domains = r.random_words(2, include_parts_of_speech=['noun'])\n",
    "    df['domain'] = [domains[0].capitalize()] * (n_asvs // 2) + [domains[1].capitalize()] * (n_asvs - n_asvs // 2)\n",
    "\n",
    "    for i in range(1, len(levels)): # Iterate over the phylogenetic levels in order of highest to lowest (skipping domain).\n",
    "        prev_level, curr_level = levels[i - 1], levels[i]\n",
    "        # Iterate over each name in the previous taxonomic level. \n",
    "        for t in set(df[prev_level].values):\n",
    "            names = r.random_words(n, regex='\\w+', include_parts_of_speech=['noun']) # Generate a list of n names for lower-level taxonomical category. \n",
    "            if curr_level != 'species': # For levels other than species, capitalize the name. \n",
    "                names = [name.capitalize() for name in names]\n",
    "            n_t = df[prev_level].str.match(t).sum() # Get the number of ASVs matching the taxon in the previous level.\n",
    "            df.loc[df[prev_level].str.match(t), curr_level] = [random.choice(names) for _ in range(n_t)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artificial_counts(n_asvs:int, n_samples:int, n_zeros:int=1000):\n",
    "    '''Generate an artificial count matrix for n_asvs ASVs and n_samples samples. The count matrix is then converted to\n",
    "    a melted pandas DataFrame to make merging with taxonomy data easier.'''\n",
    "    \n",
    "    # # Maximum number of zeros to ensure that at least every sample and every ASV has at least one nonzero entry. \n",
    "    # max_zeros = (n_samples * n_asvs) - max(n_asvs, n_samples) \n",
    "    # assert n_zeros < max_zeros, f'create_artificial_count_data: Cannot be more than {max_zeros} zeros in the count matrix.'\n",
    "\n",
    "    # Requirement is to have at least one count in each ASV category and each sample. \n",
    "    counts = np.random.randint(low=1, high=5000, size=(n_samples, n_asvs), dtype=int)\n",
    "    row_idxs, col_idxs = np.arange(n_samples), np.arange(n_asvs)\n",
    "    counts[np.random.choice(row_idxs, size=n_zeros), np.random.choice(row_idxs, size=n_zeros)] = 0\n",
    "    \n",
    "    assert not np.any(counts.sum(axis=0) == 0), 'create_artificial_count_data: There are too many zeros! Try decreasing n_zeros and re-running.'\n",
    "    assert not np.any(counts.sum(axis=1) == 0), 'create_artificial_count_data: There are too many zeros! Try decreasing n_zeros and re-running.'\n",
    "\n",
    "    # Create a matrix with the count data. \n",
    "    df = pd.DataFrame(counts, index=np.arange(n_samples), columns=[f'ASV_{i}' for i in range(n_asvs)])\n",
    "    # Make sure to set ignore_index=True so that the serial codes are preserved when melting. \n",
    "    df = df.melt(ignore_index=False, value_name='count', var_name='asv')\n",
    "    df['serial_code'] = df.index # For some reason, if I don't set the serial code as a column, it gets dropped during the merge with taxonomy.\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artificial_metadata(n_samples:int, n_categorical:int=1, n_continuous:int=1):\n",
    "    '''Generate an artificial DataFrame of sample metadata with both continuous and categorical variables.'''\n",
    "    max_num_categorical = 5\n",
    "    max_val_continuous = 10\n",
    "\n",
    "    r = wonderwords.RandomWord()\n",
    "    categorical_vars = r.random_words(n_categorical, include_parts_of_speech=['adjective'])\n",
    "    continuous_vars = r.random_words(n_continuous, include_parts_of_speech=['adjective'])\n",
    "\n",
    "    df = pd.DataFrame(index=np.arange(n_samples), columns=categorical_vars + continuous_vars)\n",
    "    for var in categorical_vars:\n",
    "        df[var] = np.random.randint(low=1, high=max_num_categorical, size=n_samples) \n",
    "    for var in continuous_vars:\n",
    "        df[var] = np.random.random(size=n_samples) * max_val_continuous\n",
    "    df['serial_code'] = df.index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_asvs = 1000\n",
    "n_samples = 100\n",
    "\n",
    "artificial_taxonomy_df = create_artificial_taxonomy(n_asvs)\n",
    "artificial_counts_df = create_artificial_counts(n_asvs, n_samples)\n",
    "artificial_metadata_df = create_artificial_metadata(n_samples)\n",
    "\n",
    "artificial_counts_df.merge(artificial_taxonomy_df, on='asv', how='left').to_csv('/home/prichter/Documents/methanotrophy/tests/data.csv', index=False)\n",
    "artificial_metadata_df.to_csv('/home/prichter/Documents/methanotrophy/tests/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "methanotrophy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
