{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('./src')\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import NoReturn\n",
    "import wonderwords\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/prichter/Documents/data/methanotrophy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_from_metadata(path:str) -> pd.DataFrame:\n",
    "    '''Load the sample metadata.'''\n",
    "    metadata_df = pd.read_csv(path)\n",
    "    metadata_df.columns = [col.lower() for col in metadata_df.columns]\n",
    "    metadata_df = metadata_df[~metadata_df.serial_code.isin(['PCR blank', 'Extr blank', 'KML'])] # Drop the outliers. \n",
    "    metadata_df.serial_code = metadata_df.serial_code.apply(int) # Convert serial code to integers.\n",
    "    metadata_df.soil_depth = metadata_df.soil_depth.str.lower()\n",
    "    return metadata_df\n",
    "\n",
    "\n",
    "def df_from_taxonomy(path:str) -> pd.DataFrame:\n",
    "    '''Load the ASV taxonomy information. The taxonomy file contains non-prokaryotes (e.g. nematodes and Eukaryotes), which \n",
    "    we keep in for the same of completeness.'''\n",
    "    taxonomy_df = pd.read_csv(path, delimiter='\\t', index_col=0)\n",
    "    # Handled the unclassified taxa (they are entered as {some category}_unclassified)\n",
    "    taxonomy_df = taxonomy_df.map(lambda s : s.replace('unclassified_', ''))\n",
    "\n",
    "    for col in taxonomy_df.columns:\n",
    "        # TODO: Might need a more general way to handle the Eukaryotes. \n",
    "\n",
    "        # Some of the nematode taxonomy labels are slightly irregular, as they contain clade information. The clades are discarded\n",
    "        # for the same of simplicity (probably don't need much taxonomical resolution for the nematodes.)\n",
    "        taxonomy_df[col] = taxonomy_df[col].apply(lambda s : 'Amorphea' if ('Amorphea' in s) else s)\n",
    "        # SAR or Harosa is a highly diverse clade of eukaryotes, often considered a supergroup, that includes stramenopiles, alveolates,\n",
    "        # and rhizarians (all Eukaryotes). Also make an exception for taxonomy labels of this kind. \n",
    "        taxonomy_df[col] = taxonomy_df[col].apply(lambda s : 'Harosa' if ('SAR' in s) else s)\n",
    "        taxonomy_df[col] = taxonomy_df[col].apply(lambda s : 'Archaeplastida' if ('Archaeplastida' in s) else s)\n",
    "\n",
    "        p = '([a-zA-Z0-9_]+)_([0-9]+)' # Pattern to match, all taxa ending in _{number}{number}\n",
    "        # Create a new \"sub\" column for more taxonomical resolution if a numerical sub-category is given. \n",
    "        taxonomy_df[col + '_sub'] = taxonomy_df[col].apply(lambda s : int(re.match(p, s).group(2)) if not (re.match(p, s) is None) else 0)\n",
    "        taxonomy_df[col] = taxonomy_df[col].apply(lambda s : re.match(p, s).group(1) if not (re.match(p, s) is None) else s)\n",
    "    \n",
    "    taxonomy_df.index.name = 'asv'\n",
    "    return taxonomy_df\n",
    "\n",
    "\n",
    "def df_from_counts(path:str) -> pd.DataFrame:\n",
    "    '''Load the ASV count information.'''\n",
    "    counts_df = pd.read_csv(path, delimiter='\\t', index_col=0)\n",
    "    counts_df['asv'] = counts_df.index\n",
    "    # Convert columns to a categorical 'sample' variable. \n",
    "    counts_df = counts_df.melt(value_name='count', id_vars='asv', var_name='sample')\n",
    "    # Throw out the weird samples.\n",
    "    counts_df = counts_df[~counts_df['sample'].isin(['HDK-DNAexNegLot169030916-30cyc', 'HDK-MAR-PCR-BLANK'])]\n",
    "    return counts_df.set_index('asv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples HDK22-KML-sand-dry, HDK22-KML-sand-wet are present in the counts.tsv file, but not in metadata.\n",
      "Dropping 192136 rows in the counts data which do not have associated metadata.\n"
     ]
    }
   ],
   "source": [
    "counts_df = df_from_counts(os.path.join(DATA_DIR, 'counts.tsv'))\n",
    "taxonomy_df = df_from_taxonomy(os.path.join(DATA_DIR, 'taxonomy.tsv'))\n",
    "metadata_df = df_from_metadata(f'{DATA_DIR}/metadata.csv')\n",
    "\n",
    "missing_samples = set(counts_df['sample'].values) - set(metadata_df['sample'].values)\n",
    "print(f\"Samples {', '.join(list(missing_samples))} are present in the counts.tsv file, but not in metadata.\")\n",
    "print(f\"Dropping {counts_df['sample'].isin(missing_samples).values.sum()} rows in the counts data which do not have associated metadata.\" )\n",
    "counts_df = counts_df[~counts_df['sample'].isin(missing_samples)] # Remove rows in counts_df which don't have associated metadata.\n",
    "\n",
    "sample_to_serial_code_map = {row.sample:row.serial_code for row in metadata_df.itertuples()}\n",
    "counts_df['serial_code'] = counts_df['sample'].apply(lambda sample : sample_to_serial_code_map[sample])\n",
    "counts_df = counts_df.drop(columns='sample')\n",
    "\n",
    "# Combine the data across files into a single DataFrame. Drop the samples column, which is redundant with the serial code. \n",
    "df = counts_df.merge(taxonomy_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Save the cleaned-up data to a CSV file. \n",
    "df.to_csv(f'{DATA_DIR}/data.csv') # 'asv' is the index, so make sure to include when writing. \n",
    "metadata_df.to_csv(f'{DATA_DIR}/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial data\n",
    "\n",
    "For testing the functionality of functions included in this software package, it is also necessary to generate some psuedo-count data, as well as artificial metadata. The artificial taxa counts are sampled from a normal distribution. Random taxa names for each taxonomy level (domain, kingdom, phylum, class, order, family, genus, species) are also generated. Metadata for each sample are also randomly generated, containing both categorical and continuous fake environmental variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artificial_taxonomy(n_asvs:int, n:int=3):\n",
    "    '''Generate an artificial hierarchical taxonomy for n_asvs ASVs. The input n\n",
    "    specifies the number of branches from each node in the artificial hierarchy.'''\n",
    "    r = wonderwords.RandomWord() # Instantiate the random word generator. \n",
    "\n",
    "    levels = ['domain', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "    df = pd.DataFrame(columns=['asv'] + levels) # Instantiate a DataFrame with the necessary columns.     \n",
    "    df['asv'] = [f'ASV_{i}' for i in range(n_asvs)] # Add the ASVs to the DataFrame. \n",
    "\n",
    "    # Assign domains to each ASV to initialize the hierarchy. \n",
    "    domains = r.random_words(2, include_parts_of_speech=['noun'])\n",
    "    df['domain'] = [domains[0].capitalize()] * (n_asvs // 2) + [domains[1].capitalize()] * (n_asvs - n_asvs // 2)\n",
    "\n",
    "    for i in range(1, len(levels)): # Iterate over the phylogenetic levels in order of highest to lowest (skipping domain).\n",
    "        prev_level, curr_level = levels[i - 1], levels[i]\n",
    "        # Iterate over each name in the previous taxonomic level. \n",
    "        for t in set(df[prev_level].values):\n",
    "            names = r.random_words(n, regex='\\w+', include_parts_of_speech=['noun']) # Generate a list of n names for lower-level taxonomical category. \n",
    "            if curr_level != 'species': # For levels other than species, capitalize the name. \n",
    "                names = [name.capitalize() for name in names]\n",
    "            n_t = df[prev_level].str.match(t).sum() # Get the number of ASVs matching the taxon in the previous level.\n",
    "            df.loc[df[prev_level].str.match(t), curr_level] = [random.choice(names) for _ in range(n_t)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artificial_counts(n_asvs:int, n_samples:int, n_zeros:int=1000):\n",
    "    '''Generate an artificial count matrix for n_asvs ASVs and n_samples samples. The count matrix is then converted to\n",
    "    a melted pandas DataFrame to make merging with taxonomy data easier.'''\n",
    "    \n",
    "    # # Maximum number of zeros to ensure that at least every sample and every ASV has at least one nonzero entry. \n",
    "    # max_zeros = (n_samples * n_asvs) - max(n_asvs, n_samples) \n",
    "    # assert n_zeros < max_zeros, f'create_artificial_count_data: Cannot be more than {max_zeros} zeros in the count matrix.'\n",
    "\n",
    "    # Requirement is to have at least one count in each ASV category and each sample. \n",
    "    counts = np.random.randint(low=1, high=5000, size=(n_samples, n_asvs), dtype=int)\n",
    "    row_idxs, col_idxs = np.arange(n_samples), np.arange(n_asvs)\n",
    "    counts[np.random.choice(row_idxs, size=n_zeros), np.random.choice(row_idxs, size=n_zeros)] = 0\n",
    "    \n",
    "    assert not np.any(counts.sum(axis=0) == 0), 'create_artificial_count_data: There are too many zeros! Try decreasing n_zeros and re-running.'\n",
    "    assert not np.any(counts.sum(axis=1) == 0), 'create_artificial_count_data: There are too many zeros! Try decreasing n_zeros and re-running.'\n",
    "\n",
    "    # Create a matrix with the count data. \n",
    "    df = pd.DataFrame(counts, index=np.arange(n_samples), columns=[f'ASV_{i}' for i in range(n_asvs)])\n",
    "    # Make sure to set ignore_index=True so that the serial codes are preserved when melting. \n",
    "    df = df.melt(ignore_index=False, value_name='count', var_name='asv')\n",
    "    df['serial_code'] = df.index # For some reason, if I don't set the serial code as a column, it gets dropped during the merge with taxonomy.\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artificial_metadata(n_samples:int, n_categorical:int=1, n_continuous:int=1):\n",
    "    '''Generate an artificial DataFrame of sample metadata with both continuous and categorical variables.'''\n",
    "    max_num_categorical = 5\n",
    "    max_val_continuous = 10\n",
    "\n",
    "    r = wonderwords.RandomWord()\n",
    "    categorical_vars = r.random_words(n_categorical, include_parts_of_speech=['adjective'])\n",
    "    continuous_vars = r.random_words(n_continuous, include_parts_of_speech=['adjective'])\n",
    "\n",
    "    df = pd.DataFrame(index=np.arange(n_samples), columns=categorical_vars + continuous_vars)\n",
    "    for var in categorical_vars:\n",
    "        df[var] = np.random.randint(low=1, high=max_num_categorical, size=n_samples) \n",
    "    for var in continuous_vars:\n",
    "        df[var] = np.random.random(size=n_samples) * max_val_continuous\n",
    "    df['serial_code'] = df.index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_asvs = 1000\n",
    "n_samples = 100\n",
    "\n",
    "artificial_taxonomy_df = create_artificial_taxonomy(n_asvs)\n",
    "artificial_counts_df = create_artificial_counts(n_asvs, n_samples)\n",
    "artificial_metadata_df = create_artificial_metadata(n_samples)\n",
    "\n",
    "artificial_counts_df.merge(artificial_taxonomy_df, on='asv', how='left').to_csv('/home/prichter/Documents/methanotrophy/tests/data.csv', index=False)\n",
    "artificial_metadata_df.to_csv('/home/prichter/Documents/methanotrophy/tests/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "methanotrophy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
